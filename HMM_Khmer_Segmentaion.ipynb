{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMM-Khmer-Segmentaion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g0sDq1FdAjpc"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIzTSxIYGxtn",
        "colab_type": "text"
      },
      "source": [
        "## Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mwyuZ_GC3jo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This code below is mainly from\n",
        "https://github.com/jwchennlp/Chinese-Word-segmentation with small modification.\n",
        "\n",
        "Other notes and references:\n",
        "* https://nickchenyj.files.wordpress.com/2012/12/automatic-chinese-word-segmentation-with-hidden-markov-models-final.pdf -- not clear eval critera got 69% to 94%\n",
        "* A Pragmatic Approach for Classical Chinese Word Segmentation -- more recent include CRF- F-Score 76%\n",
        "https://www.aclweb.org/anthology/L18-1186\n",
        "* https://www.aclweb.org/anthology/W10-4128 HMM Revises Low Marginal Probability by CRF\n",
        "for Chinese Word Segmentation 97% accuracy\n",
        "* https://people.cs.umass.edu/~mccallum/papers/chineseseg.pdf - Chinese Word Segmentation with\n",
        "Conditional Random Fields and Integrated Domain Knowledge-- got 97% good paper to read on crf\n",
        "* https://github.com/keithnull/ChineseWordSegmentationSystem (not using the code, no model source code -- has flask for web )\n",
        "pku_test: 0.763 --> 0.829(the latest version)\n",
        "msr_test: 0.793 --> 0.889(the latest version) -- using F1 Score\n",
        "* http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.7060&rep=rep1&type=pdf - JAPANESE WORD SEGMENTATION BY HIDDEN MARKOV MODEL  -- 91% (https://www.aclweb.org/anthology/H94-1054)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEYaqfIKGl4k",
        "colab_type": "text"
      },
      "source": [
        "## Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxPpX3-W98_G",
        "colab_type": "text"
      },
      "source": [
        "Emission Matrix A: \n",
        "\n",
        "$P(X_k = w|Y_k = t) = A_{t,w}, \\forall k$\n",
        "\n",
        "| tag\\char | a   | e   | h   | i   | s   | t   |\n",
        "|      --- | --- | --- | --- | --- | --- | --- |\n",
        "| S        | 0.8 | --  | --  | --  | --  | --  |\n",
        "| B        | --  | --  | --  | 0.5 | --  | 0.4 |\n",
        "| M        | --  | 0.9 | 0.9 | 0.4 | 0.4 | --  |\n",
        "| E        | --  | --  | --  | --  | 0.3 | 0.5 |\n",
        "\n",
        "Transition Matrix B:\n",
        "\n",
        "$P(Y_k = t|Y_{k-1} = s) = B_{s,t}, \\forall k$\n",
        "\n",
        "| tag\\tag | S  | B  | M  | E  |\n",
        "|      -- | -- | -- | -- | -- |\n",
        "| S       | 0.1| 0.8| -- | -- |\n",
        "| B       | -- | -- | 0.5| 0.4|\n",
        "| M       | -- | -- | 0.4| 0.5|\n",
        "| E       | 0.3| 0.4| -- | -- |\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ_7p7WvwygU",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pzRH1IYPh5h",
        "colab_type": "code",
        "outputId": "2294c16a-1450-4651-d698-9582eb3e007b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Download the file from `url` and save it to outfile\n",
        "import urllib.request\n",
        "size = [\"100\",\"500\",\"1000\",\"5000\",\"10000_seg\"]\n",
        "# set file size for list of available size (0-5) that you want to use\n",
        "docsize = size[0]\n",
        "data_dir = \"kh_data_\" + docsize\n",
        "file_name = data_dir + \".zip\"\n",
        "base_url = \"https://github.com/phylypo/segmentation-crf-khmer/raw/master/data/\"\n",
        "url = base_url + file_name\n",
        "print(\"Downloading from:\", url)\n",
        "urllib.request.urlretrieve(url, file_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from: https://github.com/phylypo/segmentation-crf-khmer/raw/master/data/kh_data_100.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('kh_data_100.zip', <http.client.HTTPMessage at 0x7f9c7ab2f630>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uX8sfRn1Uf4",
        "colab_type": "code",
        "outputId": "d42ae7eb-eeaf-4cca-a370-0cf6af0c2caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(\"- Unzipping the file and show last few extracted files:\")\n",
        "!unzip {file_name} | tail -10\n",
        "\n",
        "print(\"- Count the number of files:\")\n",
        "!ls -al {data_dir}/*_seg.txt | wc -l\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Unzipping the file and show last few extracted files:\n",
            "  inflating: kh_data_100/313540_seg.txt  \n",
            "  inflating: kh_data_100/313541_orig.txt  \n",
            "  inflating: kh_data_100/313541_seg.txt  \n",
            "  inflating: kh_data_100/313544_orig.txt  \n",
            "  inflating: kh_data_100/313544_seg.txt  \n",
            "  inflating: kh_data_100/313545_orig.txt  \n",
            "  inflating: kh_data_100/313545_seg.txt  \n",
            "  inflating: kh_data_100/313546_orig.txt  \n",
            "  inflating: kh_data_100/313546_seg.txt  \n",
            "  inflating: kh_data_100/meta.txt    \n",
            "- Count the number of files:\n",
            "100\n",
            "kh_data_100  kh_data_100.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhfp40P2RXJl",
        "colab_type": "code",
        "outputId": "dea69791-3426-4897-ba69-21bf2752be27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Combine the content of files into one file for training and test\n",
        "#!ls -alh kh_data_100/313[34]*_seg.txt\n",
        "!cat kh_data_100/313[34]*_seg.txt > khmer_seg_train.txt\n",
        "!cat kh_data_100/3135*_seg.txt > khmer_seg_test.txt\n",
        "#!head khmer_seg_train.txt\n",
        "\n",
        "#total line: 982 , 35K words\n",
        "!wc khmer_seg_train.txt      # line count: 799, words: 28,287\n",
        "!wc khmer_seg_test.txt # line count: 182, words:  7,072"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   799  28287 416448 khmer_seg_train.txt\n",
            "   183   7072 101702 khmer_seg_test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kptZmh3GPt1X",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpbwSDQ0CjKz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class Model(object):\n",
        "    def __init__(self,states,observation,phi,trans_prob,conf_prob):\n",
        "        self._states = states\n",
        "        self._observation = observation\n",
        "        self._phi = phi\n",
        "        self._trans_prob = trans_prob\n",
        "        self._conf_prob = conf_prob\n",
        "\n",
        "    def states_length(self):\n",
        "        #Return the length of the states\n",
        "        return len(self._states)\n",
        "\n",
        "    def _forward(self,observations):\n",
        "        #The implemention of the forward algorithm\n",
        "        s_len = self.states_length\n",
        "        o_len = len(observations)\n",
        "        '''\n",
        "        This step should cal the alpha_t(j)\n",
        "        the t is the length of the observations,\n",
        "        the j is the hidden states\n",
        "        '''\n",
        "        alpha = [[] for i in range(o_len)]\n",
        "        \n",
        "        alpha[0] = {}\n",
        "        #t=1,cal the intil alpha_1(j)\n",
        "        for state in self._states:\n",
        "            alpha[0][state] = self._conf_prob[state][observations[0]]*self._phi[state]\n",
        "        \n",
        "        #t>1,cal the local prob alpha_t(j)\n",
        "        for index in range(1,o_len):\n",
        "            alpha[index] ={}\n",
        "            for state_to in self._states:\n",
        "                #the time t the prob all path that direct to states_to\n",
        "                prob = 0\n",
        "                for state_from in self._states:\n",
        "                    prob += alpha[index-1][state_from]*self._trans_prob[state_from][state_to]\n",
        "                alpha[index][state_to]=self._conf_prob[state_to][observations[index]]*prob\n",
        "        return alpha\n",
        "        \n",
        "    def _backward(self,observations):\n",
        "        #The implementation of the backward algorithm\n",
        "        s_len = self.states_length\n",
        "        o_len = len(observations)\n",
        "        '''\n",
        "        This step should cal the beta_t(j)\n",
        "        the t is the location of the observations,\n",
        "        the j is the hidden states\n",
        "        beta_t(j) = p(o_(t+1)...o_T|q_t=s_j,\\lambda)\n",
        "        '''\n",
        "        beta = [[] for i in range(o_len)] \n",
        "        beta[o_len-1] = {}\n",
        "        #t=T,the intial beta_T(j)\n",
        "        for state in self._states:\n",
        "            beta[o_len-1][state] = 1\n",
        "        \n",
        "        #t<T,cal the local prob beta_t(j)\n",
        "        index = len(observations)-1\n",
        "        while index > 0:\n",
        "            beta[index-1] = {}\n",
        "            for state_from in self._states:\n",
        "                prob = 0\n",
        "                for state_to in self._states:\n",
        "                    prob += self._trans_prob[state_from][state_to] * \\\n",
        "                        self._conf_prob[state_to][observations[index]]* \\\n",
        "                        beta[index][state_to]\n",
        "                beta[index-1][state_from] = prob\n",
        "            index -= 1\n",
        "        return beta\n",
        "        \n",
        "    def _viterbi(self,observations):\n",
        "        #The implemention of the viterbi algorithm\n",
        "        s_len = self.states_length\n",
        "        o_len = len(observations)\n",
        "        '''\n",
        "        This step should cal the beta_t(j),\n",
        "        the t is the length of the observations,\n",
        "        the j is the hidden states,\n",
        "        the beta_t(j) means at time t the most probable \n",
        "        local path to state j\n",
        "        '''\n",
        "        beta = [[] for i in range(o_len)]\n",
        "        beta[0] = {}\n",
        "        \n",
        "        for state in self._states:\n",
        "            beta[0][state] = self._conf_prob[state][observations[0]]*self._phi[state]\n",
        "            \n",
        "        #t>1,cal the local prob beta_t(j)\n",
        "        for index in range(1,o_len):\n",
        "            beta[index] = {}\n",
        "            for state_to in self._states:\n",
        "                #build a list to save the beta_t-1(j)a_jib_ikt\n",
        "                prob = []\n",
        "                for state_from in self._states:\n",
        "                    temp = beta[index-1][state_from]*self._trans_prob[state_from][state_to]*self._conf_prob[state_to][observations[index]]\n",
        "                    prob.append(temp)\n",
        "                prob =sorted(prob,reverse = True)\n",
        "                beta[index][state_to] = prob[0]\n",
        "        return beta\n",
        "    \n",
        "    def _backward_point(self,beta,observations,state):\n",
        "        \"\"\"\n",
        "        rely on the beta to get the state sequences that best \n",
        "        explain the observation sequences\n",
        "        \"\"\"\n",
        "        index = len(observations)-1\n",
        "        theta =[0 for i in range(len(observations))]\n",
        "        theta[index] = state\n",
        "        while index >0:\n",
        "            prob = {}\n",
        "            for state_from in self._states:\n",
        "                prob[state_from] = beta[index-1][state_from]*self._trans_prob[state_from][state]\n",
        "            state = sorted(prob,key=prob.get,reverse=True)[0]\n",
        "            index -= 1\n",
        "            theta[index] = state\n",
        "        return theta\n",
        "        \n",
        "    def _inverse(self,beta):\n",
        "        result = [0 for i in range(len(beta))] \n",
        "        length = len(beta)\n",
        "        for i in range(len(beta)):\n",
        "            result[i] = beta[length-i-1]\n",
        "        return result\n",
        "    \n",
        "    def _intial_par(self):\n",
        "        '''\n",
        "        phi,trans_prob,conf_prob = {},{},{}\n",
        "        N = len(self._states)\n",
        "        M = len(self._observation)\n",
        "        for state in self._states:\n",
        "            phi[state] = 1.0/N\n",
        "            trans_prob[state] = {}\n",
        "            for state_to in self._states:\n",
        "                trans_prob[state][state_to] = 1.0/N\n",
        "            conf_prob[state] = {}\n",
        "            for ob in self._observation:\n",
        "                conf_prob[state][ob] = 1.0/M\n",
        "        '''\n",
        "        phi = self._phi\n",
        "        trans_prob = self._trans_prob\n",
        "        conf_prob = self._conf_prob\n",
        "        return (phi,trans_prob,conf_prob)\n",
        "\n",
        "    def _cal_gamma(self,alpha,beta,observations):\n",
        "        T = len(observations)\n",
        "        gamma = [[] for x in range(T)]\n",
        "        for t in range(T):\n",
        "            gamma[t] = {}\n",
        "            sum_prob = 0\n",
        "            for state in self._states:\n",
        "                prob = alpha[t][state]*beta[t][state]\n",
        "                sum_prob += prob\n",
        "                gamma[t][state] = prob\n",
        "            for state in self._states:\n",
        "                if gamma[t][state] == 0:\n",
        "                    continue\n",
        "                else:\n",
        "                    gamma[t][state] /= sum_prob\n",
        "        return gamma\n",
        "        \n",
        "    def _cal_espi(self,alpha,beta,trans_prob,conf_prob,observations):\n",
        "        T = len(observations)\n",
        "        espi = [[] for x in range(T-1)]\n",
        "        for t in range(T-1):\n",
        "            espi[t] = {}\n",
        "            sum_prob = 0\n",
        "            for state_i in self._states:\n",
        "                espi[t][state_i] = {}\n",
        "                for state_j in self._states:\n",
        "                   prob = alpha[t][state_i]*trans_prob[state_i][state_j]*conf_prob[state_j][observations[t+1]]*beta[t+1][state_j]\n",
        "                   espi[t][state_i][state_j] = prob\n",
        "                   sum_prob += prob\n",
        "            for i in self._states:\n",
        "                for j in self._states:\n",
        "                    if espi[t][i][j] == 0:\n",
        "                        continue\n",
        "                    else:\n",
        "                        espi[t][i][j] /= sum_prob\n",
        "        return espi\n",
        "        \n",
        "    def _evaluate_par(self,gamma,espi,observations):\n",
        "        T = len(observations)\n",
        "        phi = gamma[0]\n",
        "        trans_prob,conf_prob = {},{}\n",
        "        for state in self._states:\n",
        "            trans_prob[state] = {}\n",
        "            conf_prob[state] = {}\n",
        "        for i in self._states:\n",
        "            for j in self._states:\n",
        "                gamma_t,espi_t = 0,0\n",
        "                for t in range(T-1):\n",
        "                    espi_t += espi[t][i][j]\n",
        "                    gamma_t += gamma[t][i]\n",
        "                trans_prob[i][j] = espi_t/gamma_t\n",
        "        for state in self._states:\n",
        "            for o in self._observation:\n",
        "                gamma_con_t ,gamma_t = 0,0\n",
        "                for t in range(T):\n",
        "                    if observations[t] == o:\n",
        "                        gamma_con_t += gamma[t][state]\n",
        "                    gamma_t = gamma[t][state]\n",
        "                conf_prob[state][o] = gamma_con_t/gamma_t\n",
        "        return (phi,trans_prob,conf_prob)\n",
        "\n",
        "    \n",
        "    def evaluate(self,observations):\n",
        "        \"\"\"\n",
        "        use the forward algorithm to cal the \n",
        "        prob of the observation sequence under the HMM Model\n",
        "        \"\"\"\n",
        "        length = len(observations)\n",
        "        if length == 0:\n",
        "            return 0\n",
        "        \n",
        "        alpha = self._forward(observations)\n",
        "        prob = sum(alpha[length-1].values())\n",
        "        return prob\n",
        "        \n",
        "    def decode(self,observations):\n",
        "        \"\"\"\n",
        "        user the be viterbi algorithm to cal the most probable \n",
        "        hidden state sequence to the observations sequence ,\n",
        "        \"\"\"\n",
        "        length = len(observations)\n",
        "        if length == 0 :\n",
        "            return 0\n",
        "        beta = self._viterbi(observations)\n",
        "        #get the last state to the last obseravtions\n",
        "        sequence = beta[length-1]\n",
        "        state = sorted(sequence,key=sequence.get,reverse=True)[0]\n",
        "        theta = self._backward_point(beta,observations,state)\n",
        "        return theta\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz5RdDEJDGtC",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVB6CtDPDKKL",
        "colab_type": "code",
        "outputId": "6ee194be-3628-4cff-e138-8077f59f0bdf",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title pre-process code -expand to see detail\n",
        "import codecs,re\n",
        "import sys\n",
        "\n",
        "class Process(object):\n",
        "    def __init__(self,file_dir,S):\n",
        "        self._file_dir = file_dir\n",
        "        self._S = S\n",
        "        self.labels =[]\n",
        "\n",
        "    def _str2words(self,test):\n",
        "        words =[]\n",
        "        x=codecs.lookup(\"utf-8\")\n",
        "        for string in test:\n",
        "            word = x.decode(string[0])[0]\n",
        "            words.append(word)\n",
        "        return words\n",
        "\n",
        "    def _statics(self):\n",
        "        f = codecs.open(self._file_dir,'rb',encoding = 'utf-8')\n",
        "        hidden_states,train = [],[]\n",
        "        for line in f.readlines():\n",
        "            '''\n",
        "            First make tag for the tokenize in the corpus\n",
        "            '''\n",
        "            hidden_state = ''\n",
        "            words = []\n",
        "            # clean up text -- remove 2 spaces to 1, and invisible spaces\n",
        "            line = line.strip()\n",
        "            line = line.replace(\"  \",\" \")\n",
        "            line = line.replace('\\u200b','') \n",
        "            line = line.replace('\\r\\n','')\n",
        "            tokenizes = line.split()\n",
        "            for token in tokenizes:\n",
        "                length = len(token)\n",
        "                if length == 1:\n",
        "                    hidden_state += 'S'\n",
        "                elif length==2:\n",
        "                    hidden_state += 'BE'\n",
        "                else:\n",
        "                    hidden_state += 'B'+(length-2)*'M'+'E'\n",
        "            '''\n",
        "            Second we should extart single character from the corpus\n",
        "            '''\n",
        "            line = line.replace(' ','') # remove space\n",
        "            #print(\"--line:\", line)\n",
        "            for word in line:\n",
        "                words.append(word) # this is character, not word\n",
        "                # can use kcc here\n",
        "            if len(words) >0:\n",
        "                train.append(words)\n",
        "                hidden_states.append(hidden_state)\n",
        "        print(\"process._statics: \",self._file_dir, \" total word count:\", len(train), \" len hidden_state:\", len(hidden_states))\n",
        "        print(\"process._statics: \",self._file_dir, \" first word/line:\", train[0], \" len hidden_state:\", len(hidden_states))\n",
        "        return (hidden_states,train)\n",
        "            \n",
        "    def _statics_hidden(self):\n",
        "        '''\n",
        "        First,get the tokenize result of the corpus,\n",
        "        statics the hidden state of each word\n",
        "        '''\n",
        "        f = open(self._file_dir,'rb')\n",
        "        hidden_states,train = [],[]\n",
        "        regex=re.compile(\"(?x) ( [\\w-]+ | [\\x80-\\xff]{3} )\")\n",
        "        for line in f.readlines():\n",
        "            hidden_state = ''\n",
        "            words = []\n",
        "            tokenizes = line.split()\n",
        "            for token in tokenizes:\n",
        "                temp = [w for w in regex.split(token) if w]\n",
        "                for t in temp:\n",
        "                    words.append(t)\n",
        "                length = len(temp)\n",
        "                if length == 1:\n",
        "                    hidden_state += 'S'\n",
        "                elif length==2:\n",
        "                    hidden_state += 'BE'\n",
        "                else:\n",
        "                    hidden_state += 'B'+(length-2)*'M'+'E'\n",
        "            if len(words) != 0:\n",
        "                train.append(words)\n",
        "                hidden_states.append(hidden_state)\n",
        "        return (hidden_states,train)\n",
        "\n",
        "            \n",
        "    def _word_count(self,train):\n",
        "        word_count = {}\n",
        "        for words in train:\n",
        "            for word in words:\n",
        "                if word in word_count.keys(): #word_count.has_key(word):\n",
        "                    word_count[word] += 1\n",
        "                else:\n",
        "                    word_count[word] = 1\n",
        "        return word_count\n",
        "    \n",
        "    def _convert(self,hidden_states):\n",
        "        temp = []\n",
        "        for index in range(len(hidden_states)):\n",
        "            regex = re.compile(\"(\\w{1})\")\n",
        "            states = [w for w in regex.split(hidden_states[index]) if w]\n",
        "            if len(states) !=0:\n",
        "                temp.append(states)\n",
        "        return temp\n",
        "    \n",
        "    def _cal_trans(self,h_s):\n",
        "        trans_prob,state_count = {},{}\n",
        "        #intial\n",
        "        for state in self._S:\n",
        "            trans_prob[state]={}\n",
        "            state_count[state] = 0\n",
        "            for state_i in self._S:\n",
        "                trans_prob[state][state_i]=0\n",
        "        for i in range(len(h_s)):\n",
        "            length = len(h_s[i])\n",
        "            for j in range(length-1):\n",
        "                s_from = h_s[i][j]\n",
        "                s_to = h_s[i][j+1]\n",
        "                trans_prob[s_from][s_to] += 1\n",
        "                state_count[s_from] += 1\n",
        "            state_count[h_s[i][length-1]] += 1\n",
        "        print(state_count)\n",
        "        for i in self._S:\n",
        "            for j in self._S:\n",
        "                trans_prob[i][j] /= float(state_count[i])\n",
        "        return (trans_prob,state_count)\n",
        "    \n",
        "    def _cal_conf(self,h_s,test_wordcount,word_count,train,state_count):\n",
        "        conf_prob = {}\n",
        "        words = list(set(word_count.keys())|set(test_wordcount.keys()))\n",
        "        print('The corpus has distinct count %d word'%(len(words)))\n",
        "        for state in self._S:\n",
        "            conf_prob[state] = {}\n",
        "            for word in words:\n",
        "                conf_prob[state][word] = 1\n",
        "        for i in range(len(h_s)):\n",
        "            length = len(h_s[i])\n",
        "            for j in range(length):\n",
        "                obser = train[i][j]\n",
        "                hidden = h_s[i][j]\n",
        "                conf_prob[hidden][obser] += 1\n",
        "        for state in self._S:\n",
        "            for word in words:\n",
        "                if conf_prob[state][word] == 0:\n",
        "                    continue\n",
        "                else:\n",
        "                    conf_prob[state][word] /= float(state_count[state])\n",
        "        return conf_prob\n",
        "        \n",
        "\n",
        "    def _tran_conf_prob(self,train,test_wordcount,word_count,hidden_states):\n",
        "        #convert the hidden_state string to list\n",
        "        hidden_states = self._convert(hidden_states)\n",
        "        trans_prob,state_count = self._cal_trans(hidden_states)\n",
        "        conf_prob = self._cal_conf(hidden_states,test_wordcount,word_count,train,state_count)\n",
        "        \n",
        "        return (conf_prob,trans_prob)\n",
        "        \n",
        "    def _word_sequence(self,test,o_hstate):\n",
        "        sequence = []\n",
        "        f= open('result.txt','w')\n",
        "        print('word_seq len test:', len(test))\n",
        "        for i in range(len(test)):\n",
        "            if o_hstate[i][-1] == 'M':\n",
        "                o_hstate[i][-1] = 'E'\n",
        "            elif o_hstate[i][-1] == 'B':\n",
        "                o_hstate[i][-1] = 'S'\n",
        "            length = len(test[i])\n",
        "            temp = []\n",
        "            k = 0\n",
        "            #print(\"len(test[i]\",len(test[i]), \"len(o_hstate[i])\",len(o_hstate[i]))\n",
        "            while k < length:\n",
        "                if o_hstate[i][k]=='S':\n",
        "                    temp.append(test[i][k])\n",
        "                else :\n",
        "                    s=test[i][k]\n",
        "                    k+=1\n",
        "                    try:\n",
        "                      if k < len(o_hstate[i]): \n",
        "                        #add by phyl to check invalid index\n",
        "                        while k < len(o_hstate[i])-1 and o_hstate[i][k] != 'E' :\n",
        "                          s += test[i][k]\n",
        "                          k +=1\n",
        "                        s += test[i][k]\n",
        "                        temp.append(s)\n",
        "                    except: \n",
        "                      print(\"exception i:\",i, \"k:\", k, \"len(o_hstate)\", len(o_hstate), \"len(o_hstate[i])\", len(o_hstate[i]))\n",
        "                k += 1\n",
        "            f.write('%s\\n' %(' '.join(temp)))\n",
        "            #print(i, \" - len(temp):\", len(temp), (' - '.join(temp)))\n",
        "            sequence.append(' '.join(temp))\n",
        "        f.close()\n",
        "            \n",
        "        return sequence\n",
        "      \n",
        "print(\"[DONE]\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DONE]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLIyNfwltVWV",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9D8OkeiDZDg",
        "colab_type": "code",
        "outputId": "9ce91fbe-ae1e-4e86-ee89-55c02a45191e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import codecs\n",
        "import sys\n",
        "\n",
        "#train_dir = '/content/icwb2-data/training/pku_training.utf8'\n",
        "#test_dir = '/content/icwb2-data/testing/pku_test.utf8'\n",
        "train_dir = 'khmer_seg_train.txt'\n",
        "test_dir = 'khmer_seg_test.txt'\n",
        "\n",
        "'''\n",
        "The number of the hidden states\n",
        "B:a word at the start\n",
        "E:a word at the end\n",
        "M:a word at the middle\n",
        "S:a word construct the tokenize\n",
        "'''\n",
        "\n",
        "S = ['B','E','M','S']\n",
        "pro = Process(train_dir,S)\n",
        "hidden_states,train=pro._statics()\n",
        "\n",
        "pro_test = Process(test_dir,S)\n",
        "test_states,test = pro_test._statics()\n",
        "\n",
        "test_wordcount = pro_test._word_count(test)\n",
        "word_count = pro._word_count(train)\n",
        "\n",
        "observation = word_count.keys()\n",
        "print(\"test state len:\",len(test_states), \"test states0\",test_states[0])\n",
        "print(\"Observation:\", len(observation), observation)\n",
        "\n",
        "'''\n",
        "The conf_prob is the probability of a observation in condition of a hidden state\n",
        "The trans_prob is the probability of a  hidden state trans to another\n",
        "This time add the smoothing method.\n",
        "1.add 1 mehtod\n",
        "'''\n",
        "conf_prob,trans_prob=pro._tran_conf_prob(train,test_wordcount,word_count,hidden_states)\n",
        "\n",
        "print('conf_prob', conf_prob)\n",
        "print('trans_prob', trans_prob)\n",
        "\n",
        "observations = test\n",
        "\n",
        "phi = {'B':0.4,'E':0.4,'M':0.1,'S':0.1} #Begin, End, Middle, Single\n",
        "model = Model(S,observation,phi,trans_prob,conf_prob)\n",
        "o_hstate = []\n",
        "\n",
        "print(\"- Preprocess: len of observation:\", len(observations), observations[0])\n",
        "for obser in observations:\n",
        "    '''\n",
        "    Notice,if a setence is too long,when we use viterbi algorithm it may result in the beta = 0\n",
        "    There are two solution,one is split the setence into serval sub_setence,another is use log function for the viterbi \n",
        "    here we select the first method\n",
        "    '''\n",
        "    length = len(obser)\n",
        "    index,sub_obser,state= 0,[],[]\n",
        "    # end of sentence -- we already break by sentence to new line, this is not necceary\n",
        "    END_TOKENS = ['ã€‚', ',','áŸ•','áŸ”','?',')',\":\",\"\\\"\"]\n",
        "    while index < length:\n",
        "        sub_obser.append(obser[index])\n",
        "        if obser[index] in END_TOKENS: #obser[index] == 'ã€‚' or obser[index]=='ï¼Œ':\n",
        "            sub_state = model.decode(sub_obser)\n",
        "            sub_obser = []\n",
        "            state += sub_state\n",
        "        elif index == length-1:\n",
        "            sub_state = model.decode(sub_obser)\n",
        "            sub_obser = []\n",
        "            state += sub_state\n",
        "        index += 1\n",
        "    o_hstate.append(state)\n",
        "\n",
        "word_sequence = pro._word_sequence(observations,o_hstate)\n",
        "print(\"-word_sequence[0]\", word_sequence[0])\n",
        "print(\"0-hstates0\", o_hstate[0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "process._statics:  khmer_seg_train.txt  total word count: 799  len hidden_state: 799\n",
            "process._statics:  khmer_seg_train.txt  first word/line: ['ážƒ', 'áž¶', 'áž', 'áŸ‹', 'áž‡', 'áž“', 'ážŸ', 'áž„', 'áŸ’', 'ážŸ', 'áŸ', 'áž™', 'áž˜', 'áŸ’', 'áž“', 'áž¶', 'áž€', 'áŸ‹', 'áž”', 'áž“', 'áŸ’', 'áž‘', 'áž¶', 'áž”', 'áŸ‹', 'áž–', 'áž¸', 'áž’', 'áŸ’', 'ážœ', 'áž¾', 'ážŸ', 'áž€', 'áž˜', 'áŸ’', 'áž˜', 'áž—', 'áž¶', 'áž–', 'áž›', 'áž½', 'áž…', 'áž™', 'áž€', 'áž€', 'áž¶', 'áž”', 'áž¼', 'áž”', 'áž›', 'áž»', 'áž™', 'áž‡', 'áž“', 'ážš', 'áž„', 'áž‚', 'áŸ’', 'ážš', 'áŸ„', 'áŸ‡', 'áž€', 'áŸ’', 'áž“', 'áž»', 'áž„', 'áž•', 'áŸ’', 'ážŸ', 'áž¶', 'ážš', 'áž›', 'áž¾', 'áž’', 'áŸ†', 'áž', 'áŸ’', 'áž˜', 'áž¸', 'áž€', 'áŸ’', 'ážš', 'áž»', 'áž„', 'ážŸ', 'áŸ€', 'áž˜', 'ážš', 'áž¶', 'áž”', 'áž“', 'áž·', 'áž„', 'áž”', 'áž¶', 'áž“', 'ážŠ', 'áž€', 'áž ', 'áž¼', 'áž', 'áž”', 'áž¶', 'áž“', 'áž‘', 'áŸ’', 'ážš', 'áž–', 'áŸ’', 'áž™', 'ážŸ', 'áž˜', 'áŸ’', 'áž”', 'áž', 'áŸ’', 'áž', 'áž·', 'áž‡', 'áž¼', 'áž“', 'áž‡', 'áž“', 'ážš', 'áž„', 'áž‚', 'áŸ’', 'ážš', 'áŸ„', 'áŸ‡', 'ážœ', 'áž·', 'áž‰']  len hidden_state: 799\n",
            "process._statics:  khmer_seg_test.txt  total word count: 183  len hidden_state: 183\n",
            "process._statics:  khmer_seg_test.txt  first word/line: ['áž¢', 'áž¶', 'áž˜', 'áŸ', 'ážš', 'áž·', 'áž€', 'áž–', 'áŸ’', 'ážš', 'áž˜', 'áž•', 'áŸ’', 'áž', 'áž›', 'áŸ‹', 'ážŸ', 'áŸ', 'ážœ', 'áž¶', 'áž‚', 'áž¶', 'áŸ†', 'áž‘', 'áŸ’', 'ážš', 'áž”', 'áž…', 'áŸ’', 'áž…', 'áŸ', 'áž€', 'áž‘', 'áŸ', 'ážŸ', 'ážŠ', 'áž›', 'áŸ‹', 'áž™', 'áž“', 'áŸ’', 'áž', 'áž ', 'áŸ„', 'áŸ‡', 'áž…', 'áž˜', 'áŸ’', 'áž”', 'áž¶', 'áŸ†', 'áž„', 'F', '-', '1', '6', 'áž”', 'áŸ‰', 'áž¶', 'áž‚', 'áž¸', 'ážŸ', 'áŸ’', 'áž', 'áž¶', 'áž“', 'áž“', 'áž·', 'áž„', 'áž›', 'áž€', 'áŸ‹', 'áž‚', 'áŸ’', 'ážš', 'áž¿', 'áž„', 'áž”', 'áž“', 'áŸ’', 'áž›', 'áž¶', 'ážŸ', 'áŸ‹', 'áž™', 'áž“', 'áŸ’', 'áž', 'áž ', 'áŸ„', 'áŸ‡', 'áž™', 'áŸ„', 'áž’', 'áž¶', 'áž²', 'áŸ’', 'áž™', 'áž¥', 'ážŽ', 'áŸ’', 'ážŒ', 'áž¶', '2', '7', ',', 'J', 'u', 'l', '2', '0', '1', '9', ',', '1', '0', ':', '3', '0', 'p', 'm']  len hidden_state: 183\n",
            "test state len: 183 test states0 BMMMMMEBMMEBMMMEBMMEBMMMMEBMMMMMMMEBMEBMMMMMEBMMMMMESSBEBMMMMMMMMEBMEBMEBMMMEBMMMMMEBMMMMMEBMMEBMEBMMMEBMEBMEBMMESBMMMEBE\n",
            "Observation: 167 dict_keys(['ážƒ', 'áž¶', 'áž', 'áŸ‹', 'áž‡', 'áž“', 'ážŸ', 'áž„', 'áŸ’', 'áŸ', 'áž™', 'áž˜', 'áž€', 'áž”', 'áž‘', 'áž–', 'áž¸', 'áž’', 'ážœ', 'áž¾', 'áž—', 'áž›', 'áž½', 'áž…', 'áž¼', 'áž»', 'ážš', 'áž‚', 'áŸ„', 'áŸ‡', 'áž•', 'áŸ†', 'áž', 'áŸ€', 'áž·', 'ážŠ', 'áž ', 'áž‰', ':', 'áŸƒ', 'áŸ¢', 'áŸ¤', 'áž', 'áŸ‚', 'áž†', 'áŸ ', 'áŸ¡', 'áŸ©', 'áŸ', 'áŸ‰', 'áŸ¨', 'áž¹', 'ážŽ', 'ážˆ', 'áŸŠ', 'áž¢', 'áŸ¥', 'áŸ¦', 'áŸ…', 'áŸˆ', '/', 'áŸ”', 'áž±', 'áŸ', 'áž²', 'áŸ', 'áž‹', 'áŸ£', 'áž¡', '+', 'ážº', 'áž§', 'áž¯', 'ážŒ', 'áž¿', 'áŸ§', 'áŸ–', '-', 'áŸ•', 'áž', 'áŸŒ', '(', 'W', 'T', 'O', ')', 'Â«', 'áŸ—', '!', 'Â»', 'áž¬', 'G', '2', '0', 'áž¥', 'M', 'a', 't', 'i', 'n', 'P', 'l', 'b', 'h', 'U', 'r', 'd', 'e', 's', 'V', 'S', 'v', 'â€', 'o', 'B', 'C', 'A', 'g', 'c', 'u', 'p', 'J', ',', '5', '4', '1', '.', '\"', 'ážª', 'm', 'R', 'f', 'F', 'L', 'H', 'w', 'k', 'y', 'j', 'â€œ', 'K', 'I', 'N', 'D', 'â€¦', 'â€“', 'X', '_', 'E', '?', '$', '9', 'Z', '8', '7', '6', '%', '3', 'Y', 'áž«', 'z', 'Ã·', 'â€™', 'Â©', '=', 'áž¦', 'x'])\n",
            "{'B': 26487, 'E': 26487, 'M': 75403, 'S': 1800}\n",
            "The corpus has distinct count 176 word\n",
            "conf_prob {'B': {'áŸ': 3.775437006833541e-05, 'v': 0.00015101748027334164, 'áž·': 3.775437006833541e-05, 'áž': 3.775437006833541e-05, 'áŸŠ': 3.775437006833541e-05, 'áŸ©': 0.00011326311020500623, '3': 3.775437006833541e-05, 'áŸ¢': 0.008381470155170462, 'áž‚': 0.03175142522747008, 'áž„': 0.0010948767319817268, '9': 0.00018877185034167705, 'ážˆ': 0.004303998187790236, 'áŸ–': 3.775437006833541e-05, '$': 3.775437006833541e-05, 'g': 3.775437006833541e-05, '+': 3.775437006833541e-05, 'áž': 0.028240268811114886, 'áž›': 0.02986370672405331, 'ážƒ': 0.004945822478951939, 'Z': 3.775437006833541e-05, 'áž–': 0.03771661569826707, 'áž†': 0.009325329406878847, '6': 7.550874013667082e-05, 'áž˜': 0.06093555329029335, 'f': 7.550874013667082e-05, 'ážŸ': 0.08328614037074791, 'H': 0.0004908068108883604, 'áž²': 0.0022652622041001245, '=': 3.775437006833541e-05, 'W': 0.0005663155510250311, 'â€“': 3.775437006833541e-05, 'E': 0.00018877185034167705, 'T': 0.0009816136217767207, 'áŸ—': 3.775437006833541e-05, 'ážœ': 0.012043644051798997, '8': 0.00018877185034167705, '.': 0.0003397893306150187, 'Â»': 0.00011326311020500623, 'áŸ‹': 3.775437006833541e-05, 'áŸ': 3.775437006833541e-05, 'ážº': 3.775437006833541e-05, 'áž¦': 0.00011326311020500623, 'áŸŒ': 3.775437006833541e-05, 'â€œ': 3.775437006833541e-05, 'L': 0.0005285611809566958, 'V': 0.0003775437006833541, 'I': 0.0003020349605466833, 'áž’': 0.011741609091252313, 'Ã·': 3.775437006833541e-05, 'O': 0.00022652622041001246, 'áž»': 3.775437006833541e-05, 'u': 3.775437006833541e-05, '5': 7.550874013667082e-05, 'áž©': 3.775437006833541e-05, 'áž«': 0.00022652622041001246, 'Ã‰': 3.775437006833541e-05, 'áž—': 0.011552837240910635, 'ážŠ': 0.05410201230792464, 'áŸ…': 3.775437006833541e-05, 'áŸ¤': 0.0010193679918450561, '-': 3.775437006833541e-05, 'r': 3.775437006833541e-05, 't': 7.550874013667082e-05, 'N': 0.0005285611809566958, 'áŸ‰': 3.775437006833541e-05, '%': 3.775437006833541e-05, 'áž±': 0.0038131913769018765, 'ážš': 0.045380752822139166, 'J': 0.0006795786612300374, 'áž': 0.02771170763015819, 'e': 3.775437006833541e-05, 'áž¯': 0.0015856835428700873, ',': 3.775437006833541e-05, 'Â©': 3.775437006833541e-05, 'â€¦': 0.00015101748027334164, 'áž‰': 0.0006040699210933666, 'A': 0.0009061048816400498, 'áž½': 3.775437006833541e-05, 'áž…': 0.03356363499075018, 'X': 0.00015101748027334164, 'áŸ’': 3.775437006833541e-05, 'R': 0.0005285611809566958, 'áž ': 0.01415788877562578, 'áž¡': 0.0030581039755351682, 'áž•': 0.016725185940272586, '4': 3.775437006833541e-05, 'b': 3.775437006833541e-05, 'áŸ„': 3.775437006833541e-05, 'áž§': 0.002114244723826783, 'áž¶': 3.775437006833541e-05, 'C': 0.0007550874013667082, 'áž¹': 3.775437006833541e-05, '/': 3.775437006833541e-05, 'P': 0.0012081398421867331, 'áŸ': 3.775437006833541e-05, 'ážŒ': 0.0002642805904783479, '0': 0.00015101748027334164, 'z': 3.775437006833541e-05, 'áŸ£': 0.0009061048816400498, 'o': 3.775437006833541e-05, 'áž­': 3.775437006833541e-05, 'Â«': 3.775437006833541e-05, '7': 0.00011326311020500623, 'â€™': 3.775437006833541e-05, '2': 0.0007550874013667082, 'áž¸': 3.775437006833541e-05, 'G': 0.000641824291161702, '\"': 3.775437006833541e-05, '?': 3.775437006833541e-05, ')': 0.0002642805904783479, 'áŸ': 3.775437006833541e-05, 'ážª': 0.0002642805904783479, 'áŸƒ': 3.775437006833541e-05, 'w': 0.00015101748027334164, 'áŸ¡': 0.0032468758258768454, 'áŸ¥': 0.0009061048816400498, 'a': 7.550874013667082e-05, 'S': 0.0010571223619133916, 'áž': 0.02986370672405331, 'ážŽ': 0.003926454487106883, 'áŸ§': 0.0004530524408200249, 'áž€': 0.08789217351908483, '_': 3.775437006833541e-05, 'y': 7.550874013667082e-05, 'áž‘': 0.04100124589421226, 'n': 0.00015101748027334164, '1': 0.00018877185034167705, 'd': 0.00022652622041001246, 'áž¿': 3.775437006833541e-05, 'l': 3.775437006833541e-05, 'k': 0.0003397893306150187, 'B': 0.0007928417714350436, 'áž“': 0.07935968588364103, 'D': 0.0002642805904783479, 'áž¥': 0.0012081398421867331, 'M': 0.0009438592517083853, '!': 3.775437006833541e-05, 'áž¾': 3.775437006833541e-05, 'áŸ†': 3.775437006833541e-05, 'áž¢': 0.04379506927926907, 'áž‹': 0.0002642805904783479, 'U': 0.0003020349605466833, 'Q': 3.775437006833541e-05, 'x': 3.775437006833541e-05, ':': 3.775437006833541e-05, 'h': 0.0003397893306150187, 'áž¼': 3.775437006833541e-05, 'áŸ ': 0.001283648582323404, 'c': 0.0003397893306150187, 'áŸˆ': 3.775437006833541e-05, 'j': 7.550874013667082e-05, 'Ã©': 3.775437006833541e-05, 'áŸ¨': 0.0003775437006833541, 'áŸ”': 3.775437006833541e-05, '{': 3.775437006833541e-05, 'm': 3.775437006833541e-05, 'áž™': 0.012194661532072337, 'q': 3.775437006833541e-05, 'áž¬': 3.775437006833541e-05, 'áŸ‚': 3.775437006833541e-05, 'áŸ¦': 0.0006795786612300374, 'áŸ‡': 3.775437006833541e-05, 'p': 0.0003020349605466833, 'áŸ•': 3.775437006833541e-05, 'i': 0.000641824291161702, 'áŸ€': 3.775437006833541e-05, 'K': 0.0002642805904783479, 'Y': 0.0003397893306150187, 'â€': 0.0002642805904783479, \"'\": 3.775437006833541e-05, 'ðŸ˜­': 3.775437006833541e-05, 'áž”': 0.09665118737493865, 'F': 0.000641824291161702, 'áž‡': 0.037943141918677085, 's': 7.550874013667082e-05, '(': 3.775437006833541e-05}, 'E': {'áŸ': 0.0073243477932570696, 'v': 0.0002642805904783479, 'áž·': 0.008494733265375467, 'áž': 0.0005285611809566958, 'áŸŠ': 3.775437006833541e-05, 'áŸ©': 0.0031713670857401745, '3': 3.775437006833541e-05, 'áŸ¢': 0.0008683505115717144, 'áž‚': 0.0029825952353984973, 'áž„': 0.10371125457771738, '9': 0.00041529807075168953, 'ážˆ': 3.775437006833541e-05, 'áŸ–': 7.550874013667082e-05, '$': 3.775437006833541e-05, 'g': 0.0007928417714350436, '+': 3.775437006833541e-05, 'áž': 0.037037037037037035, 'áž›': 0.03866047494997546, 'ážƒ': 0.0003397893306150187, 'Z': 7.550874013667082e-05, 'áž–': 0.010306943028655566, 'áž†': 3.775437006833541e-05, '6': 3.775437006833541e-05, 'áž˜': 0.029335145543096613, 'f': 3.775437006833541e-05, 'ážŸ': 0.01925472873485106, 'H': 3.775437006833541e-05, 'áž²': 3.775437006833541e-05, '=': 3.775437006833541e-05, 'W': 3.775437006833541e-05, 'â€“': 3.775437006833541e-05, 'E': 0.00015101748027334164, 'T': 3.775437006833541e-05, 'áŸ—': 3.775437006833541e-05, 'ážœ': 0.01249669649261902, '8': 0.00011326311020500623, '.': 0.0004530524408200249, 'Â»': 3.775437006833541e-05, 'áŸ‹': 0.08823196284969985, 'áŸ': 7.550874013667082e-05, 'ážº': 0.00641824291161702, 'áž¦': 3.775437006833541e-05, 'áŸŒ': 0.0024540340544418017, 'â€œ': 3.775437006833541e-05, 'L': 0.00011326311020500623, 'V': 0.00011326311020500623, 'I': 0.00011326311020500623, 'áž’': 0.004530524408200249, 'Ã·': 3.775437006833541e-05, 'O': 0.0003775437006833541, 'áž»': 0.004303998187790236, 'u': 0.00022652622041001246, '5': 0.00018877185034167705, 'áž©': 3.775437006833541e-05, 'áž«': 3.775437006833541e-05, 'Ã‰': 3.775437006833541e-05, 'áž—': 0.0009816136217767207, 'ážŠ': 0.0009438592517083853, 'áŸ…': 0.022275078340317892, 'áŸ¤': 0.0011703854721183977, '-': 3.775437006833541e-05, 'r': 0.000641824291161702, 't': 0.0009061048816400498, 'N': 0.00018877185034167705, 'áŸ‰': 0.00011326311020500623, '%': 3.775437006833541e-05, 'áž±': 3.775437006833541e-05, 'ážš': 0.04198285951598898, 'J': 3.775437006833541e-05, 'áž': 0.000641824291161702, 'e': 0.002151999093895118, 'áž¯': 0.0003775437006833541, ',': 0.0003397893306150187, 'Â©': 3.775437006833541e-05, 'â€¦': 3.775437006833541e-05, 'áž‰': 0.01555480046815419, 'A': 0.0003020349605466833, 'áž½': 0.00022652622041001246, 'áž…': 0.013780345074942425, 'X': 0.00011326311020500623, 'áŸ’': 3.775437006833541e-05, 'R': 7.550874013667082e-05, 'áž ': 0.0003775437006833541, 'áž¡': 7.550874013667082e-05, 'áž•': 3.775437006833541e-05, '4': 0.00011326311020500623, 'b': 7.550874013667082e-05, 'áŸ„': 0.0009061048816400498, 'áž§': 3.775437006833541e-05, 'áž¶': 0.060067202778721636, 'C': 0.00015101748027334164, 'áž¹': 3.775437006833541e-05, '/': 3.775437006833541e-05, 'P': 0.00022652622041001246, 'áŸ': 0.009136557556537169, 'ážŒ': 0.0004908068108883604, '0': 0.0003397893306150187, 'z': 3.775437006833541e-05, 'áŸ£': 0.0009816136217767207, 'o': 0.0008683505115717144, 'áž­': 3.775437006833541e-05, 'Â«': 3.775437006833541e-05, '7': 7.550874013667082e-05, 'â€™': 3.775437006833541e-05, '2': 0.00015101748027334164, 'áž¸': 0.0593876241174916, 'G': 0.00015101748027334164, '\"': 3.775437006833541e-05, '?': 3.775437006833541e-05, ')': 0.00015101748027334164, 'áŸ': 0.010306943028655566, 'ážª': 3.775437006833541e-05, 'áŸƒ': 0.011326311020500622, 'w': 0.00015101748027334164, 'áŸ¡': 0.0020764903537584477, 'áŸ¥': 0.0009061048816400498, 'a': 0.0007173330312983728, 'S': 0.00011326311020500623, 'áž': 0.005965190470796995, 'ážŽ': 0.001925472873485106, 'áŸ§': 0.002491788424510137, 'áž€': 0.05327141616642126, '_': 3.775437006833541e-05, 'y': 0.0006040699210933666, 'áž‘': 0.004417261297995243, 'n': 0.0008683505115717144, '1': 0.00018877185034167705, 'd': 0.0005285611809566958, 'áž¿': 0.0003397893306150187, 'l': 0.0007550874013667082, 'k': 0.0007550874013667082, 'B': 0.0003397893306150187, 'áž“': 0.10295616717635066, 'D': 0.00018877185034167705, 'áž¥': 3.775437006833541e-05, 'M': 3.775437006833541e-05, '!': 3.775437006833541e-05, 'áž¾': 0.01340280137425907, 'áŸ†': 0.024464831804281346, 'áž¢': 0.0016234379129384227, 'áž‹': 0.0026805602748518143, 'U': 7.550874013667082e-05, 'Q': 3.775437006833541e-05, 'x': 3.775437006833541e-05, ':': 0.00015101748027334164, 'h': 0.0007550874013667082, 'áž¼': 0.0025295427945784725, 'áŸ ': 0.003775437006833541, 'c': 0.00022652622041001246, 'áŸˆ': 0.003435647676218522, 'j': 3.775437006833541e-05, 'Ã©': 3.775437006833541e-05, 'áŸ¨': 0.0006040699210933666, 'áŸ”': 0.00018877185034167705, '{': 3.775437006833541e-05, 'm': 0.0006795786612300374, 'áž™': 0.05791520368482652, 'q': 3.775437006833541e-05, 'áž¬': 3.775437006833541e-05, 'áŸ‚': 0.01200588968173066, 'áŸ¦': 0.0010193679918450561, 'áŸ‡': 0.03971759731188885, 'p': 0.0003775437006833541, 'áŸ•': 0.0003397893306150187, 'i': 0.0005663155510250311, 'áŸ€': 3.775437006833541e-05, 'K': 0.00015101748027334164, 'Y': 3.775437006833541e-05, 'â€': 0.00011326311020500623, \"'\": 3.775437006833541e-05, 'ðŸ˜­': 3.775437006833541e-05, 'áž”': 0.014459923736172463, 'F': 7.550874013667082e-05, 'áž‡': 0.0007173330312983728, 's': 0.0007928417714350436, '(': 3.775437006833541e-05}, 'M': {'áŸ': 0.0001326207180085673, 'v': 0.00010609657440685384, 'áž·': 0.03160351710144159, 'áž': 0.0005570070156359826, 'áŸŠ': 0.004389745766083577, 'áŸ©': 0.0001326207180085673, '3': 3.978621540257019e-05, 'áŸ¢': 0.00011935864620771057, 'áž‚': 0.007148256700661777, 'áž„': 0.019773749055077384, '9': 2.652414360171346e-05, 'ážˆ': 0.0005702690874368394, 'áŸ–': 1.326207180085673e-05, '$': 1.326207180085673e-05, 'g': 0.00023871729241542114, '+': 1.326207180085673e-05, 'áž': 0.032372717265891277, 'áž›': 0.02546317785764492, 'ážƒ': 0.0008620346670556874, 'Z': 1.326207180085673e-05, 'áž–': 0.014203678898717558, 'áž†': 0.0004376483694282721, '6': 3.978621540257019e-05, 'áž˜': 0.028009495643409415, 'f': 9.28345026059971e-05, 'ážŸ': 0.02883174409506253, 'H': 2.652414360171346e-05, 'áž²': 0.0010609657440685384, '=': 1.326207180085673e-05, 'W': 2.652414360171346e-05, 'â€“': 1.326207180085673e-05, 'E': 7.957243080514038e-05, 'T': 0.00021219314881370767, 'áŸ—': 1.326207180085673e-05, 'ážœ': 0.01172367147195735, '8': 3.978621540257019e-05, '.': 0.0001326207180085673, 'Â»': 1.326207180085673e-05, 'áŸ‹': 0.007519594711085766, 'áŸ': 0.004774345848308423, 'ážº': 0.0010874898876702518, 'áž¦': 1.326207180085673e-05, 'áŸŒ': 0.0006631035900428365, 'â€œ': 1.326207180085673e-05, 'L': 7.957243080514038e-05, 'V': 0.00014588278980942404, 'I': 0.00021219314881370767, 'áž’': 0.00636579446441123, 'Ã·': 1.326207180085673e-05, 'O': 0.00010609657440685384, 'áž»': 0.031404586024428735, 'u': 0.0005304828720342692, '5': 7.957243080514038e-05, 'áž©': 1.326207180085673e-05, 'áž«': 1.326207180085673e-05, 'Ã‰': 1.326207180085673e-05, 'áž—': 0.00636579446441123, 'ážŠ': 0.010052650425049402, 'áŸ…': 0.002652414360171346, 'áŸ¤': 0.00010609657440685384, '-': 1.326207180085673e-05, 'r': 0.0008620346670556874, 't': 0.0006631035900428365, 'N': 0.00014588278980942404, 'áŸ‰': 0.005981194382186385, '%': 1.326207180085673e-05, 'áž±': 0.0004509104412291288, 'ážš': 0.06502393803960055, 'J': 1.326207180085673e-05, 'áž': 0.0036735938888373143, 'e': 0.0016047106879036642, 'áž¯': 0.0004774345848308423, ',': 0.0003713380104239884, 'Â©': 1.326207180085673e-05, 'â€¦': 1.326207180085673e-05, 'áž‰': 0.006883015264644643, 'A': 0.0001856690052119942, 'áž½': 0.01602058273543493, 'áž…': 0.011975650836173628, 'X': 1.326207180085673e-05, 'áŸ’': 0.14666525204567457, 'R': 0.0001326207180085673, 'áž ': 0.006153601315597523, 'áž¡': 0.0024269591395567817, 'áž•': 0.0023208625651499276, '4': 1.326207180085673e-05, 'b': 0.0002652414360171346, 'áŸ„': 0.01905759717783112, 'áž§': 0.0002254552206145644, 'áž¶': 0.1199289152951474, 'C': 9.28345026059971e-05, 'áž¹': 0.007890932721509755, '/': 1.326207180085673e-05, 'P': 7.957243080514038e-05, 'áŸ': 0.00019893107701285094, 'ážŒ': 0.0014190416826916701, '0': 0.00017240693341113747, 'z': 0.00015914486161028077, 'áŸ£': 0.00014588278980942404, 'o': 0.001869952123920799, 'áž­': 1.326207180085673e-05, 'Â«': 1.326207180085673e-05, '7': 0.0001326207180085673, 'â€™': 1.326207180085673e-05, '2': 0.00010609657440685384, 'áž¸': 0.00758590507009005, 'G': 3.978621540257019e-05, '\"': 1.326207180085673e-05, '?': 1.326207180085673e-05, ')': 1.326207180085673e-05, 'áŸ': 0.026364998740103177, 'ážª': 1.326207180085673e-05, 'áŸƒ': 0.0021882418471413606, 'w': 0.0001856690052119942, 'áŸ¡': 0.0012201106056788191, 'áŸ¥': 0.0001856690052119942, 'a': 0.0019495245547259392, 'S': 6.631035900428365e-05, 'áž': 0.0040316698274604455, 'ážŽ': 0.011365595533334218, 'áŸ§': 7.957243080514038e-05, 'áž€': 0.03442833839502407, '_': 1.326207180085673e-05, 'y': 0.00017240693341113747, 'áž‘': 0.0146811134835484, 'n': 0.0012333726774796758, '1': 0.0001326207180085673, 'd': 0.0005304828720342692, 'áž¿': 0.0016975451905096614, 'l': 0.0007957243080514038, 'k': 0.00014588278980942404, 'B': 6.631035900428365e-05, 'áž“': 0.04185509860350384, 'D': 9.28345026059971e-05, 'áž¥': 1.326207180085673e-05, 'M': 3.978621540257019e-05, '!': 1.326207180085673e-05, 'áž¾': 0.01667042425367691, 'áŸ†': 0.02051642507592536, 'áž¢': 0.004482580268689575, 'áž‹': 0.0020290969855310797, 'U': 3.978621540257019e-05, 'Q': 1.326207180085673e-05, 'x': 0.00010609657440685384, ':': 0.00011935864620771057, 'h': 0.0006365794464411231, 'áž¼': 0.018195562510775434, 'áŸ ': 0.001976048698327653, 'c': 0.0005172208002334125, 'áŸˆ': 0.00042438629762741535, 'j': 3.978621540257019e-05, 'Ã©': 1.326207180085673e-05, 'áŸ¨': 0.00010609657440685384, 'áŸ”': 1.326207180085673e-05, '{': 1.326207180085673e-05, 'm': 0.0003580759386231317, 'áž™': 0.00977414691723141, 'q': 1.326207180085673e-05, 'áž¬': 1.326207180085673e-05, 'áŸ‚': 0.01698871397689747, 'áŸ¦': 0.00017240693341113747, 'áŸ‡': 0.002745248862777343, 'p': 0.0003978621540257019, 'áŸ•': 1.326207180085673e-05, 'i': 0.0012864209646831029, 'áŸ€': 0.004973276925321274, 'K': 6.631035900428365e-05, 'Y': 5.304828720342692e-05, 'â€': 1.326207180085673e-05, \"'\": 1.326207180085673e-05, 'ðŸ˜­': 1.326207180085673e-05, 'áž”': 0.03128522737822102, 'F': 0.0001856690052119942, 'áž‡': 0.011405381748736787, 's': 0.0007161518772462634, '(': 1.326207180085673e-05}, 'S': {'áŸ': 0.0005555555555555556, 'v': 0.0011111111111111111, 'áž·': 0.0005555555555555556, 'áž': 0.0005555555555555556, 'áŸŠ': 0.0005555555555555556, 'áŸ©': 0.0038888888888888888, '3': 0.0016666666666666668, 'áŸ¢': 0.029444444444444443, 'áž‚': 0.0011111111111111111, 'áž„': 0.0022222222222222222, '9': 0.0011111111111111111, 'ážˆ': 0.0005555555555555556, 'áŸ–': 0.065, '$': 0.0011111111111111111, 'g': 0.0005555555555555556, '+': 0.0011111111111111111, 'áž': 0.0033333333333333335, 'áž›': 0.0022222222222222222, 'ážƒ': 0.0005555555555555556, 'Z': 0.0011111111111111111, 'áž–': 0.006666666666666667, 'áž†': 0.0005555555555555556, '6': 0.0011111111111111111, 'áž˜': 0.005, 'f': 0.0005555555555555556, 'ážŸ': 0.015, 'H': 0.0005555555555555556, 'áž²': 0.0005555555555555556, '=': 0.0011111111111111111, 'W': 0.0005555555555555556, 'â€“': 0.0011111111111111111, 'E': 0.0011111111111111111, 'T': 0.0005555555555555556, 'áŸ—': 0.055, 'ážœ': 0.0005555555555555556, '8': 0.0011111111111111111, '.': 0.012222222222222223, 'Â»': 0.01888888888888889, 'áŸ‹': 0.0005555555555555556, 'áŸ': 0.0005555555555555556, 'ážº': 0.0005555555555555556, 'áž¦': 0.0005555555555555556, 'áŸŒ': 0.0005555555555555556, 'â€œ': 0.01, 'L': 0.0005555555555555556, 'V': 0.0022222222222222222, 'I': 0.0005555555555555556, 'áž’': 0.0005555555555555556, 'Ã·': 0.0011111111111111111, 'O': 0.0005555555555555556, 'áž»': 0.0005555555555555556, 'u': 0.0005555555555555556, '5': 0.005555555555555556, 'áž©': 0.0005555555555555556, 'áž«': 0.0005555555555555556, 'Ã‰': 0.0005555555555555556, 'áž—': 0.0005555555555555556, 'ážŠ': 0.0005555555555555556, 'áŸ…': 0.0005555555555555556, 'áŸ¤': 0.01, '-': 0.02277777777777778, 'r': 0.0005555555555555556, 't': 0.0005555555555555556, 'N': 0.0005555555555555556, 'áŸ‰': 0.0005555555555555556, '%': 0.005, 'áž±': 0.0005555555555555556, 'ážš': 0.0022222222222222222, 'J': 0.0005555555555555556, 'áž': 0.0005555555555555556, 'e': 0.0005555555555555556, 'áž¯': 0.0022222222222222222, ',': 0.02666666666666667, 'Â©': 0.0016666666666666668, 'â€¦': 0.021666666666666667, 'áž‰': 0.0005555555555555556, 'A': 0.0011111111111111111, 'áž½': 0.0005555555555555556, 'áž…': 0.006111111111111111, 'X': 0.0005555555555555556, 'áŸ’': 0.0005555555555555556, 'R': 0.0005555555555555556, 'áž ': 0.0005555555555555556, 'áž¡': 0.0005555555555555556, 'áž•': 0.0005555555555555556, '4': 0.0011111111111111111, 'b': 0.0005555555555555556, 'áŸ„': 0.0005555555555555556, 'áž§': 0.0005555555555555556, 'áž¶': 0.0005555555555555556, 'C': 0.0005555555555555556, 'áž¹': 0.0005555555555555556, '/': 0.002777777777777778, 'P': 0.0005555555555555556, 'áŸ': 0.0005555555555555556, 'ážŒ': 0.0005555555555555556, '0': 0.0005555555555555556, 'z': 0.0005555555555555556, 'áŸ£': 0.012222222222222223, 'o': 0.0005555555555555556, 'áž­': 0.0005555555555555556, 'Â«': 0.017222222222222222, '7': 0.0011111111111111111, 'â€™': 0.0011111111111111111, '2': 0.0005555555555555556, 'áž¸': 0.0005555555555555556, 'G': 0.006111111111111111, '\"': 0.0011111111111111111, '?': 0.01611111111111111, ')': 0.043333333333333335, 'áŸ': 0.0005555555555555556, 'ážª': 0.0005555555555555556, 'áŸƒ': 0.0005555555555555556, 'w': 0.0005555555555555556, 'áŸ¡': 0.028333333333333332, 'áŸ¥': 0.012777777777777779, 'a': 0.0005555555555555556, 'S': 0.0005555555555555556, 'áž': 0.0016666666666666668, 'ážŽ': 0.0005555555555555556, 'áŸ§': 0.0077777777777777776, 'áž€': 0.005, '_': 0.0016666666666666668, 'y': 0.0005555555555555556, 'áž‘': 0.0005555555555555556, 'n': 0.0005555555555555556, '1': 0.0044444444444444444, 'd': 0.0005555555555555556, 'áž¿': 0.0005555555555555556, 'l': 0.0005555555555555556, 'k': 0.0005555555555555556, 'B': 0.0005555555555555556, 'áž“': 0.002777777777777778, 'D': 0.0005555555555555556, 'áž¥': 0.0005555555555555556, 'M': 0.0005555555555555556, '!': 0.011111111111111112, 'áž¾': 0.0005555555555555556, 'áŸ†': 0.0005555555555555556, 'áž¢': 0.0011111111111111111, 'áž‹': 0.0005555555555555556, 'U': 0.0005555555555555556, 'Q': 0.0005555555555555556, 'x': 0.0005555555555555556, ':': 0.01, 'h': 0.0005555555555555556, 'áž¼': 0.0005555555555555556, 'áŸ ': 0.0011111111111111111, 'c': 0.0005555555555555556, 'áŸˆ': 0.0005555555555555556, 'j': 0.0005555555555555556, 'Ã©': 0.0005555555555555556, 'áŸ¨': 0.008333333333333333, 'áŸ”': 0.35777777777777775, '{': 0.0005555555555555556, 'm': 0.0005555555555555556, 'áž™': 0.0022222222222222222, 'q': 0.0005555555555555556, 'áž¬': 0.02666666666666667, 'áŸ‚': 0.0005555555555555556, 'áŸ¦': 0.007222222222222222, 'áŸ‡': 0.0005555555555555556, 'p': 0.0005555555555555556, 'áŸ•': 0.034444444444444444, 'i': 0.0011111111111111111, 'áŸ€': 0.0005555555555555556, 'K': 0.0005555555555555556, 'Y': 0.0005555555555555556, 'â€': 0.01, \"'\": 0.0005555555555555556, 'ðŸ˜­': 0.0005555555555555556, 'áž”': 0.0011111111111111111, 'F': 0.0005555555555555556, 'áž‡': 0.0005555555555555556, 's': 0.0011111111111111111, '(': 0.04833333333333333}}\n",
            "trans_prob {'B': {'B': 0.0, 'E': 0.11998338807716993, 'M': 0.8800166119228301, 'S': 0.0}, 'E': {'B': 0.9377430437573149, 'E': 0.0, 'M': 0.0, 'S': 0.05968965907803828}, 'M': {'B': 0.0, 'E': 0.3091256316061695, 'M': 0.6908743683938304, 'S': 0.0}, 'S': {'B': 0.5, 'E': 0.0, 'M': 0.0, 'S': 0.09388888888888888}}\n",
            "- Preprocess: len of observation: 183 ['áž¢', 'áž¶', 'áž˜', 'áŸ', 'ážš', 'áž·', 'áž€', 'áž–', 'áŸ’', 'ážš', 'áž˜', 'áž•', 'áŸ’', 'áž', 'áž›', 'áŸ‹', 'ážŸ', 'áŸ', 'ážœ', 'áž¶', 'áž‚', 'áž¶', 'áŸ†', 'áž‘', 'áŸ’', 'ážš', 'áž”', 'áž…', 'áŸ’', 'áž…', 'áŸ', 'áž€', 'áž‘', 'áŸ', 'ážŸ', 'ážŠ', 'áž›', 'áŸ‹', 'áž™', 'áž“', 'áŸ’', 'áž', 'áž ', 'áŸ„', 'áŸ‡', 'áž…', 'áž˜', 'áŸ’', 'áž”', 'áž¶', 'áŸ†', 'áž„', 'F', '-', '1', '6', 'áž”', 'áŸ‰', 'áž¶', 'áž‚', 'áž¸', 'ážŸ', 'áŸ’', 'áž', 'áž¶', 'áž“', 'áž“', 'áž·', 'áž„', 'áž›', 'áž€', 'áŸ‹', 'áž‚', 'áŸ’', 'ážš', 'áž¿', 'áž„', 'áž”', 'áž“', 'áŸ’', 'áž›', 'áž¶', 'ážŸ', 'áŸ‹', 'áž™', 'áž“', 'áŸ’', 'áž', 'áž ', 'áŸ„', 'áŸ‡', 'áž™', 'áŸ„', 'áž’', 'áž¶', 'áž²', 'áŸ’', 'áž™', 'áž¥', 'ážŽ', 'áŸ’', 'ážŒ', 'áž¶', '2', '7', ',', 'J', 'u', 'l', '2', '0', '1', '9', ',', '1', '0', ':', '3', '0', 'p', 'm']\n",
            "word_seq len test: 183\n",
            "-word_sequence[0] áž¢áž¶áž˜áŸážšáž·áž€ * áž–áŸ’ážšáž˜ * áž•áŸ’ážáž›áŸ‹ * ážŸáŸážœáž¶ * áž‚áž¶áŸ† * áž‘áŸ’ážš * áž”áž…áŸ’áž…áŸáž€ * áž‘áŸážŸ * ážŠáž›áŸ‹ * áž™áž“áŸ’áž * áž áŸ„áŸ‡ * áž…áž˜áŸ’áž”áž¶áŸ†áž„ * F * - * 1 * 6 * áž”áŸ‰áž¶áž‚áž¸ * ážŸáŸ’ážáž¶áž“ * áž“áž·áž„ * áž›áž€áŸ‹ * áž‚áŸ’ážšáž¿áž„ * áž”áž“áŸ’áž›áž¶ážŸáŸ‹ * áž™áž“áŸ’áž * áž áŸ„áŸ‡ * áž™áŸ„áž’áž¶áž²áŸ’áž™ * áž¥ážŽáŸ’ážŒáž¶2 * 7 * , * Jul * 2019 * , * 1 * 0 * : * 3 * 0pm\n",
            "0-hstates0 ['B', 'M', 'M', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'E', 'B', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'M', 'E', 'S', 'S', 'S', 'S', 'B', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'E', 'S', 'S', 'B', 'M', 'E', 'B', 'M', 'M', 'E', 'S', 'S', 'S', 'S', 'S', 'B', 'M', 'E']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEdxP6Pt8yiC",
        "colab_type": "code",
        "outputId": "0eec9ad6-0e2b-45e0-ea68-8510756ef75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# compare to segmentation text\n",
        "!grep \"áž¢áž¶áž˜áŸážšáž·áž€ áž–áŸ’ážšáž˜ áž•áŸ’ážáž›áŸ‹\" kh_data_100/3135*_seg.txt\n",
        "#!head kh_data_100/313502_orig.txt\n",
        "!head kh_data_100/313502_seg.txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kh_data_100/313502_seg.txt: áž¢áž¶áž˜áŸážšáž·áž€ áž–áŸ’ážšáž˜ áž•áŸ’ážáž›áŸ‹ ážŸáŸážœáž¶ áž‚áž¶áŸ†áž‘áŸ’ážš áž”áž…áŸ’áž…áŸáž€áž‘áŸážŸ ážŠáž›áŸ‹ áž™áž“áŸ’ážáž áŸ„áŸ‡ áž…áž˜áŸ’áž”áž¶áŸ†áž„ F - 16 áž”áŸ‰áž¶áž‚áž¸ážŸáŸ’ážáž¶áž“ áž“áž·áž„ áž›áž€áŸ‹ áž‚áŸ’ážšáž¿áž„ áž”áž“áŸ’áž›áž¶ážŸáŸ‹ áž™áž“áŸ’ážáž áŸ„áŸ‡ áž™áŸ„áž’áž¶ áž²áŸ’áž™ áž¥ážŽáŸ’ážŒáž¶ 27, Jul 2019 , 10:30 pm \n",
            " áž¢áž¶áž˜áŸážšáž·áž€ áž–áŸ’ážšáž˜ áž•áŸ’ážáž›áŸ‹ ážŸáŸážœáž¶ áž‚áž¶áŸ†áž‘áŸ’ážš áž”áž…áŸ’áž…áŸáž€áž‘áŸážŸ ážŠáž›áŸ‹ áž™áž“áŸ’ážáž áŸ„áŸ‡ áž…áž˜áŸ’áž”áž¶áŸ†áž„ F - 16 áž”áŸ‰áž¶áž‚áž¸ážŸáŸ’ážáž¶áž“ áž“áž·áž„ áž›áž€áŸ‹ áž‚áŸ’ážšáž¿áž„ áž”áž“áŸ’áž›áž¶ážŸáŸ‹ áž™áž“áŸ’ážáž áŸ„áŸ‡ áž™áŸ„áž’áž¶ áž²áŸ’áž™ áž¥ážŽáŸ’ážŒáž¶ 27, Jul 2019 , 10:30 pm \n",
            "ážšážŠáŸ’áž‹áž¶áž—áž·áž”áž¶áž› áž€áŸ’ážšáž»áž„ ážœáŸ‰áž¶ážŸáŸŠáž¸áž“ážáŸ„áž“ áž”áž¶áž“ ážŸáž˜áŸ’ážšáŸáž… áž¢áž“áž»áž˜áŸáž áž›áž€áŸ‹ áž‚áŸ’ážšáž¿áž„ áž”áž“áŸ’áž›áž¶ážŸáŸ‹ áž“áž·áž„ áž•áŸ’ážáž›áŸ‹ áž€áž¶ážšáž‚áž¶áŸ†áž‘áŸ’ážš áž•áŸ’áž“áŸ‚áž€ áž”áž…áŸ’áž…áŸáž€áž‘áŸážŸ ážŸáž˜áŸ’ážšáž¶áž”áŸ‹ áž™áž“áŸ’ážáž áŸ„áŸ‡ áž™áŸ„áž’áž¶ F - 16 ážšáž”ážŸáŸ‹ áž”áŸ‰áž¶áž‚áž¸ážŸáŸ’ážáž¶áž“ áž“áž·áž„ áž™áž“áŸ’ážáž áŸ„áŸ‡ ážŠáž¹áž€áž‡áž‰áŸ’áž‡áž¼áž“ C - 17 ážšáž”ážŸáŸ‹ áž¥ážŽáŸ’ážŒáž¶ áŸ”\n",
            " áž™áž“áŸ’ážáž áŸ„áŸ‡ áž…áž˜áŸ’áž”áž¶áŸ†áž„ F - 16 ážšáž”ážŸáŸ‹ áž”áŸ‰áž¶áž‚áž¸ážŸáŸ’ážáž¶áž“ áž“áž·áž„ áž™áž“áŸ’ážáž áŸ„áŸ‡ ážŠáž¹áž€áž‡áž‰áŸ’áž‡áž¼áž“ áž™áŸ„áž’áž¶ C - 17 ážšáž”ážŸáŸ‹ áž¥ážŽáŸ’ážŒáž¶ áž“áž¹áž„ áž‘áž‘áž½áž›áž”áž¶áž“ áž€áž¶ážš ážŸáŸážœáž¶ áž‚áž¶áŸ†áž‘áŸ’ážš áž”áž…áŸ’áž…áŸáž€áž‘áŸážŸ áž“áž·áž„ áž—ážŸáŸ’ážáž»áž—áž¶ážš áž–áž¸ážŸáŸ†ážŽáž¶áž€áŸ‹ ážšážŠáŸ’áž‹áž”áž¶áž› áž¢áž¶áž˜áŸážšáž·áž€ áž”áž“áŸ’áž‘áž¶áž”áŸ‹áž–áž¸ áž€áŸ’ážšážŸáž½áž„ áž€áž¶ážšáž”ážšáž‘áŸážŸ áž¢áž¶áž˜áŸážšáž·áž€ ážŸáž˜áŸ’ážšáŸáž… áž¢áž“áž»áž˜áŸáž áž‚áž˜áŸ’ážšáŸ„áž„ áž˜áž¶áž“ ážáž˜áŸ’áž›áŸƒ áŸ¨áŸ áŸ  áž›áž¶áž“ ážŠáž»áž›áŸ’áž›áž¶ážš áž¢áž¶áž˜áŸážšáž·áž€ áž“áŸ… ážáŸ’áž„áŸƒážŸáž»áž€áŸ’ážš áž‘áž¸ áŸ¢áŸ¦ ážáŸ‚áž€áž€áŸ’áž€ážŠáž¶ áŸ”\n",
            " áž”áž¾ ážáž¶áž˜ áž‘áž¸áž—áŸ’áž“áž¶áž€áŸ‹áž„áž¶ážš áž–áŸážáŸŒáž˜áž¶áž“ AFP áž¢áž¶áž˜áŸážšáž·áž€ ážŸáž˜áŸ’ážšáŸáž… áž™áž›áŸ‹áž–áŸ’ážšáž˜ áž•áŸ’ážáž›áŸ‹ ážŸáŸážœáž¶ áž‚áž¶áŸ†áž‘áŸ’ážš áž”áž…áŸ’áž…áŸáž€áž‘áŸážŸ áž“áž·áž„ áž—ážŸáŸ’ážáž»áž—áž¶ážš ážŠáž›áŸ‹ áž™áž“áŸ’ážáž áŸ„áŸ‡ F - 16 ážŠáŸ‚áž› áž”áŸ‰áž¶áž‚áž¸ážŸáŸ’ážáž¶áž“ áž”áž¶áž“ áž‘áž·áž‰ áž–áž¸ áž¢áž¶áž˜áŸážšáž·áž€ áŸ”\n",
            " áž€áž¶ážšážŸáž˜áŸ’ážšáŸáž… áž“áŸáŸ‡  áž’áŸ’ážœáž¾áž¡áž¾áž„ ážáŸ’ážšáž¹áž˜ áž”áŸ‰áž»áž“áŸ’áž˜áž¶áž“ ážáŸ’áž„áŸƒ áž”áŸ‰áž»ážŽáŸ’ážŽáŸ„áŸ‡ áž€áŸ’ážšáŸ„áž™ áž‘ážŸáŸ’ážŸáž“áž€áž·áž…áŸ’áž… ážšáž”ážŸáŸ‹ áž“áž¶áž™ ážšážŠáŸ’áž‹áž˜áž“áŸ’ážáŸ’ážšáž¸ áž”áŸ‰áž¶áž‚áž¸ážŸáŸ’ážáž¶áž“ áž›áŸ„áž€ Imran Khan áž“áŸ… áž¢áž¶áž˜áŸážšáž·áž€ áž“áž·áž„ áž‡áž½áž” áž‡áž¶áž˜áž½áž™ áž›áŸ„áž€  ážŠáž¼ážŽáž¶áž›áŸ‹ ážáŸ’ážšáž¶áŸ† ážŠáŸ‚ážš áŸ”\n",
            " Â« áž€áž¶ážšáž¢áž“áž»áž˜áŸáž áž•áŸ’ážáž›áŸ‹ ážŸáŸážœáž¶ áž”áž…áŸ’áž…áŸáž€áž‘áŸážŸ áž“áž·áž„ áž—ážŸáŸ’ážáž»áž—áž¶ážš áž‘áž¶áŸ†áž„ áž“áŸáŸ‡ áž“áž¹áž„ áž‚áž¶áŸ†áž‘áŸ’ážš áž‚áŸ„áž›áž“áž™áŸ„áž”áž¶áž™ áž€áž¶ážšáž”ážšáž‘áŸážŸ áž¢áž¶áž˜áŸážšáž·áž€  áž“áž·áž„ áž’áž¶áž“áž¶ ážŸáž“áŸ’ážáž·ážŸáž»áž áž‡áž¶ážáž·  ážŠáŸ„áž™ áž€áž¶ážšáž–áž¶ážš áž”áž…áŸ’áž…áŸáž€ážœáž·áž‘áŸ’áž™áž¶ áž¢áž¶áž˜áŸážšáž·áž€ ážáž¶áž˜ážšáž™áŸˆ áž€áž¶ážšáž”áž“áŸ’áž ážšáž€áŸ’ážŸáž¶ ážœážáŸ’ážáž˜áž¶áž“ ážšáž”ážŸáŸ‹ áž¢áŸ’áž“áž€áž”áž…áŸ’áž…áŸáž€áž‘áŸážŸ áž¢áž¶áž˜áŸážšáž·áž€ áž“áŸ… áž‚áž¶áŸ†áž‘áŸ’ážš áž…áŸ’áž…áŸáž€áž‘áŸážŸ áž€áŸ’áž“áž»áž„ áž€áž¶ážšáž”áŸ’ážšáž¾áž”áŸ’ážšáž¶ážŸáŸ‹ áž™áž“áŸ’ážáž áŸ„áŸ‡ áž…áž˜áŸ’áž”áž¶áŸ†áž„ F - 16 ážšáž”ážŸáŸ‹ áž”áŸ‰áž¶áž‚áž¸ážŸáŸ’ážáž¶áž“ Â» áŸ”\n",
            " áž€áŸ’ážšážŸáž½áž„ áž€áž¶ážšáž”ážšáž‘áŸážŸ áž¢áž¶áž˜áŸážšáž·áž€ áž”áž‰áŸ’áž‡áž¶áž€áŸ‹ ážŠáž¼áž…áŸ’áž“áŸáŸ‡ áŸ”\n",
            " áž…áŸ†ážŽáŸ‚áž€ áž€áŸ’ážšážŸáž½áž„ áž€áž¶ážšáž”ážšáž‘áŸážŸ áž¢áž¶áž˜áŸážšáž·áž€ áž€áŸ ážŸáž˜áŸ’ážšáŸáž… áž¢áž“áž»áž˜áŸáž áž›áž€áŸ‹ áž‚áŸ’ážšáž¿áž„ áž”áž“áŸ’áž›áž¶ážŸáŸ‹ áž“áž·áž„ áž•áŸ’ážáž›áŸ‹ ážŸáŸážœáž¶ áž áŸ’ážœáž¹áž€áž áŸ’ážœážºáž“ áž•áŸ’ážŸáŸáž„ áž‘áŸ€áž ážŠáž¾áž˜áŸ’áž”áž¸ áž‚áž¶áŸ†áž‘áŸ’ážš áž€áž„áž‘áŸáž– áž¥ážŽáŸ’ážŒáž¶ áž€áŸ’áž“áž»áž„ áž€áž¶ážšáž”áŸ’ážšáž¾áž”áŸ’ážšáž¶ážŸáŸ‹ áž™áž“áŸ’ážáž áŸ„áŸ‡ ážŠáž¹áž€áž‡áž‰áŸ’áž‡áž¼áž“ áž™áŸ„áž’áž¶ C - 17 áŸ”\n",
            " áž‚áž˜áŸ’ážšáŸ„áž„ áž“áŸáŸ‡ áž˜áž¶áž“ ážáž˜áŸ’áž›áŸƒ ážŸážšáž»áž” áž”áŸ’ážšáž˜áž¶ážŽ áŸ¦áŸ§áŸ  áž›áž¶áž“ ážŠáž»áž›áŸ’áž›áž¶ážš áž¯ážŽáŸ„áŸ‡ áŸ”\n",
            " Â« áž¥ážŽáŸ’ážŒáž¶ ážáŸ’ážšáž¼ážœáž€áž¶ážš áž€áž¶ážšáž‚áž¶áŸ†áž‘áŸ’ážš áž”áž…áŸ’áž…áŸáž€áž‘áŸážŸ áž”áž“áŸ’áž áž‘áŸ€áž ážŠáž¾áž˜áŸ’áž”áž¸ áž’áž¶áž“áž¶ áž€áž¶ážšážáŸ’ážšáŸ€áž˜ážáŸ’áž›áž½áž“ áž”áŸ’ážšážáž·áž”ážáŸ’ážáž·áž€áž¶ážš  áž“áž·áž„ ážŸáž˜ážáŸ’ážáž—áž¶áž– áž“áž¶áž“áž¶ áž€áŸ’áž“áž»áž„ áž€áž¶ážšáž•áŸ’ážáž›áŸ‹ áž€áž¶ážšáž‚áž¶áŸ†áž‘áŸ’ážš áž”áŸ’ážšážáž·áž”ážáŸ’ážáž·áž€áž¶ážš áž•áŸ’ážáž›áŸ‹ áž‡áŸ†áž“áž½áž™ áž˜áž“áž»ážŸáŸ’ážŸáž’áž˜áŸŒ  áž“áž·áž„ ážŸáž˜áŸ’ážšáž½áž› ážŠáž›áŸ‹ áž€áž¶ážšážŠáŸ„áŸ‡ážŸáŸ’ážšáž¶áž™ áž”áž‰áŸ’áž áž¶ áž‚áŸ’ážšáŸ„áŸ‡áž˜áž áž“áŸ’ážážšáž¶áž™ áž“áŸ…áž€áŸ’áž“áž»áž„ ážáŸ†áž”áž“áŸ‹ Â» áŸ”\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isTnzrC-_-eo",
        "colab_type": "text"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0sDq1FdAjpc",
        "colab_type": "text"
      },
      "source": [
        "### Custom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81hZo3G95_bQ",
        "colab_type": "code",
        "outputId": "a579bfec-6514-4c7d-d439-1ac909b2ee19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "# Custom validation\n",
        "#prediction = [1,0,0,1,1,1,1,0,1,0]\n",
        "#correct    = [1,0,0,1,1,1,0,0,1,0]\n",
        "prediction = [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
        "correct =    [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]\n",
        "\n",
        "pstr = \"\".join(str(i) for i in prediction)\n",
        "cstr = \"\".join(str(i) for i in correct)\n",
        "print(\"P\",pstr)\n",
        "print(\"C\",cstr)\n",
        "#pl = pstr.split('1')\n",
        "\n",
        "\n",
        "def calc_perf(corrects, predictions): # list of 0/1\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "  n_correct = 0\n",
        "  n_incorrect = 0\n",
        "  total_char = 0\n",
        "  total_word = 0\n",
        "  n_correct_word = 0\n",
        "  \n",
        "  print(\"size of input:\", len(predictions), \"ground truth:\", len(corrects))\n",
        "  if len(predictions) != len(corrects): return 0\n",
        "  \n",
        "  for i, prediction in enumerate(predictions):\n",
        "    correct = corrects[i]\n",
        "    zipped = list(zip(prediction, correct))    \n",
        "    tp +=        len([1 for l, c in zipped if l == c and l == 1])\n",
        "    fp +=        len([1 for l, c in zipped if l == 1 and c == 0])\n",
        "    fn +=        len([1 for l, c in zipped if l == 0 and c == 1])\n",
        "    n_incorrect += len([1 for l, c in zipped if l != c])\n",
        "    n_correct   += len([1 for l, c in zipped if l == c])\n",
        "    #n_correct_word += len([1 for l,c in zipped if l==1 and c==1]) # not account for prediction=1 and correct=0\n",
        "    #n_incorrect_word += len([1 for l,c in zipped if l==0 and c==1]) # missing other way around\n",
        "    total_word += len([_ for l in correct if l==1])\n",
        "    total_char += len(prediction)\n",
        "    #print(\"len correct\", len(correct), \" incorrect count:\", n_incorrect)\n",
        "    # count good word\n",
        "    n_correct_word += count_correct_word(correct, prediction)\n",
        "  \n",
        "  print(\"Total char:\", str(total_char), \" total word:\", str(total_word), \"avg char/word:\", str(total_char/total_word))\n",
        "  print(\"Correct word:\" + str(n_correct_word), \" incorrect word:\", str(total_word - n_correct_word), \"word accuracy:\", n_correct_word/total_word) \n",
        "  \n",
        "  precision = tp/(tp+fp)\n",
        "  recall = tp/(tp+fn)\n",
        "  F1 = 2 * (precision * recall) / (precision + recall)\n",
        "  print(\"Precision:\\t\" + str(precision), \"tp:\", tp, \"fp:\", fp)\n",
        "  print(\"Recall:\\t\\t\" + str(recall), \"fn:\",fn)\n",
        "  print(\"F1-score:\\t\" + str(F1))\n",
        "  print(\"Accuracy:\\t\" + str(n_correct/(n_correct+n_incorrect))) \n",
        "  \n",
        "def count_correct_word(correct, prediction):\n",
        "  s = \"\"\n",
        "  for i in range(len(correct)):\n",
        "    s += \"%3s\" %str(i)\n",
        "  #print(\"prediction:\", prediction)\n",
        "  #print(\"   correct:\", correct)\n",
        "  #print(\"       str:\", s)\n",
        "  B=False\n",
        "  correct_count = 0\n",
        "  for i,c in enumerate(correct):\n",
        "    p = prediction[i]\n",
        "    nextc = -1\n",
        "    if i < len(correct)-1: \n",
        "      nextc = correct[i+1]\n",
        "    if c==1 and p==1:\n",
        "      B = True\n",
        "      correct_count += 1\n",
        "      #print(i,\"Begin word corect\", correct_count)\n",
        "    if p==0 and c==1 and B:\n",
        "      B = False\n",
        "      correct_count -= 1\n",
        "      #print(i, \"too long\")\n",
        "    if c==0 and p==1: #incorrect\n",
        "      if B: \n",
        "        correct_count -= 1\n",
        "        #print(i,\"bad word\", correct_count)\n",
        "        B = False\n",
        "  return correct_count\n",
        "\n",
        "correct_count = count_correct_word(prediction, correct)\n",
        "print(\"correct count\", correct_count)  \n",
        "calc_perf([correct], [prediction])\n",
        "\n",
        "from sklearn.metrics import classification_report \n",
        "print(classification_report(correct, prediction, target_names=[\"0\",\"1\"]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P 1000000100010000100010010010000010010010001001000000111110000100001001001000010000001000100100000010000011100100011111100\n",
            "C 1000000100010000100010000010000000010010000001000000111010000000001001001000010000001000000100010010000100100100011000010\n",
            "correct count 15\n",
            "size of input: 1 ground truth: 1\n",
            "Total char: 121  total word: 27 avg char/word: 4.481481481481482\n",
            "Correct word:15  incorrect word: 12 word accuracy: 0.5555555555555556\n",
            "Precision:\t0.6666666666666666 tp: 24 fp: 12\n",
            "Recall:\t\t0.8888888888888888 fn: 3\n",
            "F1-score:\t0.761904761904762\n",
            "Accuracy:\t0.8760330578512396\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.87      0.92        94\n",
            "           1       0.67      0.89      0.76        27\n",
            "\n",
            "    accuracy                           0.88       121\n",
            "   macro avg       0.82      0.88      0.84       121\n",
            "weighted avg       0.90      0.88      0.88       121\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0DmJUxmAnGN",
        "colab_type": "text"
      },
      "source": [
        "### Check Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBfTYO2MLnxY",
        "colab_type": "code",
        "outputId": "e2e79db7-0fae-46b6-ae3d-ac784a74b6dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "# Check accuracy\n",
        "test_labels = [] # predicted result\n",
        "g_labels = []    # ground thruth\n",
        "for i, el in enumerate(test_states):\n",
        "  test_label = []\n",
        "  for c in el: #string\n",
        "    v = 1 if c in \"BS\" else 0\n",
        "    test_label.append(v)\n",
        "  g_label = []\n",
        "  for c in o_hstate[i]:\n",
        "    v = 1 if c in \"BS\" else 0\n",
        "    g_label.append(v)\n",
        "  if len(test_label) != len(g_label):\n",
        "    print(i,\"test_label len:\", len(test_label), test_label)\n",
        "    print(i,\"   g_label len:\", len(g_label), g_label)\n",
        "    print(\"--- Not matching length ---- observation:\", observations[i])\n",
        "  test_labels.append(test_label)\n",
        "  g_labels.append(g_label)\n",
        "  \n",
        "# check custom metric\n",
        "#calc_perf(test_labels[0:1], g_labels[0:1])\n",
        "calc_perf(test_labels, g_labels)\n",
        "\n",
        "flat_predicts = [item for t in test_labels for item in t]\n",
        "flat_true = [item for t in g_labels for item in t]\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report \n",
        "print(classification_report(flat_predicts, flat_true, \n",
        "      target_names=[\"0\",\"1\"]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of input: 183 ground truth: 183\n",
            "Total char: 32179  total word: 7072 avg char/word: 4.550197963800905\n",
            "Correct word:3099  incorrect word: 3973 word accuracy: 0.43820701357466063\n",
            "Precision:\t0.49704092689839124 tp: 5963 fp: 6034\n",
            "Recall:\t\t0.8431843891402715 fn: 1109\n",
            "F1-score:\t0.625412973936756\n",
            "Accuracy:\t0.7780229342117531\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.76      0.84     25107\n",
            "           1       0.50      0.84      0.63      7072\n",
            "\n",
            "    accuracy                           0.78     32179\n",
            "   macro avg       0.72      0.80      0.73     32179\n",
            "weighted avg       0.85      0.78      0.79     32179\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}